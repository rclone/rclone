### **Intro (topic summary)**  
Weâ€™ve built a new virtual backend for **rclone** called **cRaid3**, combining three remotes into one faultâ€‘tolerant storage system. Itâ€™s an early implementation, and weâ€™d love your feedback, tests, and design input!

***

## Solving the Failing Remote Problem â€” New Virtual Backend: **cRaid3** (Request for Comments)

Dear rclone community,

Hard disks fail. That's why we have RAID â€” multiple drives working together so that when one goes down, your data stays safe and accessible.  
The same principle applies to cloud storage: an **account can get compromised**, a **provider can disappear**, or access to a **geographic region**, or even to entire organizations like **NGOs** or **companies**, can suddenly be blocked. When that happens, both current and historical data may be at risk.

To address this, we built **cloud raid3**,  **cRaid3** or in short **raid3** â€” a new **virtual backend for rclone** that combines **three remotes into one faultâ€‘tolerant storage system**.

***

### How it works

Imagine you have storage providers in the **US**, **Newâ€¯Zealand**, and **France**.  
You bundle them into a single virtual remote called `safestorage` and use it like any other remote:

```bash
$ rclone ls safestorage:
```

If the Newâ€¯Zealand provider fails, **all your data remains fully accessible for reading**.  
`safestorage` reports which backend is missing, and rebuilding uses only the data stored on the two working systems.  
You can then set up a new provider in Australia, update your `rclone.conf`, and rebuild:

```bash
$ rclone backend rebuild safestorage:
```

Thatâ€™s it â€” `safestorage` is ready for storing data again and your data is **faultâ€‘tolerant** again.

***

### Technical details

RAID3 splits data at the **byte level** across three backends:

- Evenâ€‘indexed bytes â†’ *even* remote  
- Oddâ€‘indexed bytes â†’ *odd* remote  
- XOR parity of each byte pair â†’ *parity* remote  

If one backend fails, the missing data is reconstructed from the other two:

- Missing even â†’ computed from odd XOR parity  
- Missing odd â†’ computed from even XOR parity  
- Missing parity â†’ recalculated from even XOR odd  

This provides **fault tolerance with onlyâ€¯~50â€¯%â€¯storage overhead**.

***

### Demo available

Integration test scripts and a setup helper are included:

```bash
$ cd backend/raid3/integration && ./setup.sh
$ rclone --config $(cat ${HOME}/.rclone_raid3_integration_tests.workdir)/rclone_raid3_integration_tests.config ls localraid3:
```

Make sure to `go build` and `go install` the forked rclone binary before testing.  
If you have **MinIO** running in Docker, the provided config also includes a `minioraid3` backend.

***

### ğŸ’¬ Request for feedback

This is an **alpha/experimental** implementation â€” core functionality is working and tested.  
**Current status:**
- âœ… Streaming support implemented (default: `use_streaming=true`)
- âœ… Degraded mode reads (works with 2/3 backends, automatic reconstruction)
- âœ… Automatic heal (`auto_heal=true` by default, background particle restoration)
- âœ… Auto-cleanup (`auto_cleanup=true` by default, removes orphaned objects)
- âœ… Backend commands: `status`, `rebuild`, `heal`
- âœ… Test suite: 96 PASS, 0 FAIL (fs/sync and fs/operations tests)

We'd appreciate feedback from the community, especially on issues such as:

- Should we call our new backend **cRaid3** or simply **raid3** ?  
- What should `rclone size` return â€” original data size or total across all parts?  
- How should `rclone md5sum` behave â€” should we store the original file's checksum explicitly?  
- Could the **chunker** or **crypt** virtual remote wrap the **raid3** remote?

**Known limitations:**
Update operation rollback has issues when `rollback=true` (Put and Move rollback work correctly), see [`docs/OPEN_QUESTIONS.md`](docs/OPEN_QUESTIONS.md) for details.

The implementation is available for download and testing here: **https://github.com/Breschling/rclone.git** . 

***

### Why RAID3?

RAID3 is **fast, simple, deterministic, and stateâ€‘light**.  
In traditional disk arrays, the parity disk was a bottleneck â€” but in **cloud storage this limitation doesn't exist**, making RAID3 an ideal starting point for reliable, multiâ€‘provider redundancy.

***

### Future directions: more flexibility and encryption?

As we refine raid3, we hope to identify whatâ€™s needed for stable, highâ€‘performance distributed backends in rclone.  
If the community finds this approach useful, we plan to explore more advanced (but probably more demanding) options such as **Erasureâ€¯Coding** and **Thresholdâ€¯Encryption** (see the 2021 forum topic [*â€œCan we add erasure coding to rclone?â€*](https://forum.rclone.org/t/can-we-add-erasure-coding-to-rclone/23684) between @hvrietsc (Hans) and @ncw (Nick)).

***
