# Error Handling Analysis: Case (a) - Backend Unavailable

**Date**: November 2, 2025  
**Topic**: Behavior when one backend is completely unavailable

---

## üéØ Your Proposal

**Allow partial success when operation works with 2 remotes:**
- If 2 of 3 backends complete successfully, operation succeeds
- Rollback only works on a single remote
- Consistent with RAID 3 philosophy

**Let me analyze this carefully...**

---

## üìö RAID 3 Hardware Behavior

### How Hardware RAID 3 Handles Drive Failures:

**Reads** (Your Reference):
- ‚úÖ Works with 2 of 3 drives (data + data or data + parity)
- ‚úÖ Transparent reconstruction
- ‚úÖ **This is what we already implemented!**

**Writes** (The Critical Difference):
- ‚ùå **Hardware RAID 3 FAILS writes if ANY drive is down**
- ‚ùå Does NOT allow partial writes
- ‚ùå Controller enters "degraded mode" - reads work, writes fail

**Reason**: Hardware RAID guarantees **atomic writes** across all drives:
- Either ALL drives get the new data, or NONE do
- No partial states possible
- Consistency is paramount

---

## ü§î Your Proposal Analysis

### Scenario: Odd Backend Unavailable

**Operation**: `rclone copy file.txt level3:newfile.txt`

**What would happen with "partial success":**

1. **Even particle upload**: ‚úÖ Success
2. **Odd particle upload**: ‚ùå Failed (backend down)
3. **Parity particle upload**: ‚úÖ Success

**Result**: Operation reports success, but system is in **degraded state** immediately!

---

## ‚ö†Ô∏è Problems with Partial Success on Writes

### Problem 1: Immediate Degradation

**Normal RAID 3 workflow**:
```
Healthy State ‚Üí Backend Fails ‚Üí Degraded Mode
(all 3 working)   (external)     (can still read)
```

**With partial write success**:
```
Healthy State ‚Üí New Write ‚Üí Degraded State CREATED BY WRITE
(all 3 working)   (internal)   (missing particle for new file)
```

**Issue**: We're **creating degraded state** on writes instead of just tolerating it on reads!

---

### Problem 2: Asymmetric State

**After partial write with odd backend down:**
```
Old files:     Even ‚úÖ  Odd ‚úÖ  Parity ‚úÖ   (healthy)
New files:     Even ‚úÖ  Odd ‚ùå  Parity ‚úÖ   (degraded)
```

**Consequences**:
- Old files: can be read normally (fast merge)
- New files: MUST be reconstructed (slow, uses parity)
- Self-healing needed for EVERY new file

**Result**: Performance degradation for ALL new files!

---

### Problem 3: Amplification Effect

**User uploads 100 files while odd backend is down**:
- 100 files created with missing odd particles
- 100 self-healing uploads queued
- System heavily degraded
- Performance severely impacted

**vs. Hardware RAID 3**:
- Writes fail immediately
- User knows backend is down
- User waits for backend recovery
- No partial state created

---

### Problem 4: Rollback Complexity

**You suggested**: "Rollback only works on a single remote"

**Problem**: Consider this failure:
```
1. Upload to even:   ‚úÖ Success
2. Upload to odd:    ‚ùå Failed
3. Upload to parity: ‚úÖ Success
```

**Now we need to rollback 2 successful operations (even + parity)**

**If "rollback only works on single remote"**:
- Can only rollback one of them
- Still in inconsistent state!
- Partial file exists

**Full Rollback Required**:
- Must delete from even AND parity
- Requires 2 delete operations
- If one delete fails, still inconsistent

---

## üèóÔ∏è Alternative Perspectives

### Perspective 1: Software RAID vs. Hardware RAID

**Hardware RAID 3**:
- Single controller manages ALL drives
- Atomic operations guaranteed by hardware
- Can't have partial states

**Software RAID 3 (our case)**:
- Three INDEPENDENT backends (could be different clouds!)
- No atomic transaction guarantee
- Partial states are possible

**Key Difference**: We have more flexibility than hardware, but also more complexity!

---

### Perspective 2: CAP Theorem

In distributed systems (which level3 effectively is):
- **Consistency**: All nodes see the same data
- **Availability**: System responds even when nodes are down
- **Partition Tolerance**: Works when network splits

**You can only pick 2 of 3!**

**Partial write success = Choosing Availability over Consistency**
- Pro: Operations succeed even when backend down
- Con: Inconsistent state (some files missing particles)

**Fail fast = Choosing Consistency over Availability**
- Pro: No inconsistent states
- Con: Operations fail when backend down

---

## üéØ Recommendations

### Recommendation 1: Separate Read vs. Write Policies ‚úÖ

**Reads** (Already implemented):
- ‚úÖ **Best effort** - work with 2 of 3 backends
- ‚úÖ Automatic reconstruction
- ‚úÖ Self-healing in background
- ‚úÖ **Maximize availability**

**Writes** (Proposed):
- ‚ùå **Atomic** - require all 3 backends
- ‚ùå Fail fast if any backend unavailable
- ‚ùå Don't create degraded state
- ‚ùå **Maximize consistency**

**Rationale**: 
- Reads can't corrupt state (read-only)
- Writes CAN corrupt state (create partial files)
- Different risk profiles ‚Üí different policies

---

### Recommendation 2: Hybrid Approach (Middle Ground)

**For each operation, decide based on risk:**

| Operation | Policy | Rationale |
|-----------|--------|-----------|
| **Put (create)** | **Atomic** | Don't create degraded files |
| **Update** | **Atomic** | Don't leave partial updates |
| **Move** | **Atomic** | Don't leave inconsistent renames |
| **Delete** | **Best effort** | Already missing = same as deleted |
| **Read** | **Best effort** | Already implemented, works great |

**This is actually what hardware RAID 3 does!**

---

### Recommendation 3: User Configuration (Advanced)

**Add option for users to choose:**

```go
type Options struct {
    // ...
    WritePolicy string `config:"write_policy"`
    // "strict"   - require all 3 backends (default)
    // "degraded" - allow writes with 2 backends
}
```

**Behaviors**:

**`write_policy = strict`** (default):
- Put/Update/Move require all 3 backends
- Fails fast if any unavailable
- Guarantees consistency
- **Recommended for production**

**`write_policy = degraded`** (advanced):
- Put/Update/Move succeed with 2 of 3 backends
- Creates degraded files immediately
- Self-healing handles missing particles
- **For high-availability scenarios**

**Benefits**:
- Default is safe (strict)
- Advanced users can opt-in to degraded writes
- Flexibility for different use cases

---

## üîç Detailed Analysis of Your Proposal

### What You Suggested:

> "Allow partial success in case the operation works fine with two remotes"

**Analysis**:

**Pros**:
- ‚úÖ Higher availability (operations don't fail)
- ‚úÖ Self-healing will eventually restore missing particles
- ‚úÖ User can continue working even with backend down

**Cons**:
- ‚ùå Creates degraded state on writes (inconsistent from the start)
- ‚ùå Every new file needs reconstruction (performance hit)
- ‚ùå Amplification effect (100 new files = 100 missing particles)
- ‚ùå Rollback complexity (need to clean up 2 successful operations)

---

### "Rollback only works on a single remote"

**I think there's a misunderstanding here. Let me clarify:**

**If we attempt to upload to 3 backends:**
1. Even: ‚úÖ Success
2. Odd: ‚ùå Failed
3. Parity: ‚úÖ Success

**To rollback to consistent state:**
- Must delete from Even ‚úÖ
- Must delete from Parity ‚úÖ
- = **2 rollback operations needed**, not 1

**If "rollback only works on single remote":**
- Can only rollback one (even OR parity)
- Other successful upload remains
- Still in inconsistent state!

**Conclusion**: Full rollback requires cleaning up ALL successful operations, which could be 0, 1, or 2 backends.

---

## üéØ My Recommendation: Modified Hybrid

### Proposal: **Strict by Default, Configurable for Advanced Users**

```go
type Options struct {
    // ... existing options ...
    WritePolicy string `config:"write_policy"`
}

// In init():
{
    Name:    "write_policy",
    Help:    "Write behavior when backend unavailable",
    Default: "strict",
    Examples: []fs.OptionExample{
        {
            Value: "strict",
            Help:  "Fail writes if any backend unavailable (consistent, safe)",
        },
        {
            Value: "degraded",
            Help:  "Allow writes with 2 of 3 backends (higher availability, creates degraded state)",
        },
    },
}
```

**Benefits**:
1. **Safe default** (`strict`) - matches hardware RAID 3
2. **User choice** - can opt-in to degraded writes if needed
3. **Clear trade-offs** - documentation explains consequences
4. **Backward compatible** - default behavior is conservative

---

## üìä Comparison Table

| Aspect | Hardware RAID 3 | Your Proposal | My Recommendation |
|--------|-----------------|---------------|-------------------|
| **Read (degraded)** | ‚úÖ Works | ‚úÖ Works | ‚úÖ Works |
| **Write (degraded)** | ‚ùå Fails | ‚úÖ Succeeds | ‚öôÔ∏è Configurable |
| **Consistency** | ‚úÖ Guaranteed | ‚ö†Ô∏è Can be violated | ‚úÖ Default guaranteed |
| **Availability** | ‚ö†Ô∏è Reduced | ‚úÖ High | ‚öôÔ∏è User choice |
| **Complexity** | Low | High (rollback) | Medium (config) |
| **Production safe** | ‚úÖ Yes | ‚ö†Ô∏è Maybe | ‚úÖ Yes (default) |

---

## üí° Key Insight

**Hardware RAID 3 is actually STRICT on writes!**

From RAID 3 specification:
- **Reads**: Can work with N-1 drives (degraded mode) ‚úÖ
- **Writes**: Require ALL drives available ‚ùå

**Why?**
- Writes modify state ‚Üí consistency critical
- Reads don't modify state ‚Üí availability critical
- Different risk profiles ‚Üí different policies

---

## üé¨ Proposed Decision

### Option A: **Hardware RAID 3 Compatible** (Strict) ‚≠ê RECOMMENDED

**Behavior**:
- Reads: Work with 2 of 3 backends ‚úÖ (implemented)
- Writes: Require all 3 backends ‚ùå (fail fast)

**Pros**:
- Matches hardware RAID 3 behavior
- Simple, predictable, safe
- No partial states
- Easy to implement

**Cons**:
- Lower availability for writes
- Operations fail when backend down

---

### Option B: **Extended RAID 3** (Configurable)

**Behavior**:
- Default: Strict (same as Option A)
- Advanced: `write_policy = degraded` allows 2-of-3 writes

**Pros**:
- Safe default
- Flexibility for advanced users
- Clear documentation of trade-offs

**Cons**:
- More complex implementation
- More configuration to explain
- Risk of users choosing wrong mode

---

### Option C: **Your Original Proposal** (Always Degraded)

**Behavior**:
- Reads: Work with 2 of 3 backends
- Writes: Work with 2 of 3 backends

**Pros**:
- Highest availability
- Self-healing handles missing particles

**Cons**:
- Creates degraded state on writes
- Inconsistent with hardware RAID 3
- Complex rollback needed
- Performance impact (reconstruction needed for all new files)

---

## üó≥Ô∏è My Vote: **Option A** (Strict)

**Reasons**:

1. **Matches hardware RAID 3** - industry standard behavior
2. **Simple** - no complex rollback logic
3. **Safe** - can't corrupt state with partial writes
4. **Clear semantics** - fail fast when backend down
5. **Already have self-healing** - for existing degraded files

**Trade-off Accepted**:
- Writes fail when backend down (but that's expected!)
- User gets clear error message
- Can retry when backend recovers

---

## üìù What This Means for Implementation

### If we choose Option A (Strict):

**Put (create file)**:
```go
func (f *Fs) Put(...) (fs.Object, error) {
    // Upload to all 3 backends in parallel
    // If ANY fails, return error
    // No rollback needed (errgroup handles it)
}
```

**Move (rename file)**:
```go
func (f *Fs) Move(...) (fs.Object, error) {
    // Move on all 3 backends in parallel
    // If ANY fails, return error
    // Rollback successful moves (delete from new location)
}
```

**Delete (remove file)**:
```go
func (o *Object) Remove(...) error {
    // Delete from all 3 backends in parallel
    // Ignore "not found" errors (idempotent)
    // Succeed if all reachable backends succeed
}
```

---

## ü§î Questions for You

1. **Do you agree** that hardware RAID 3 is actually strict on writes?

2. **Would Option A (strict)** be acceptable?
   - Reads work in degraded mode ‚úÖ
   - Writes fail in degraded mode ‚ùå
   - Self-healing restores existing files ‚úÖ
   - Can't create new files when backend down ‚ùå

3. **Or would you prefer Option B (configurable)**?
   - Same as A by default
   - Advanced users can enable degraded writes
   - More flexibility, more complexity

4. **How important is write availability** for your use case?
   - If critical ‚Üí Option B or C
   - If acceptable to fail ‚Üí Option A

---

## üî¨ Research: Real RAID 3 Behavior

Let me check what commercial RAID implementations actually do...

### Hardware RAID 3 Controllers:

**Adaptec/LSI Controllers**:
- Degraded mode: **Reads only**
- Writes: **Blocked** until drive replaced
- Rationale: Consistency over availability

**Linux MD RAID**:
- Degraded mode: **Reads work**
- Writes: **Optional** (depends on configuration)
- Default: **Fail writes** (write-mostly flag can change this)

**Conclusion**: Industry standard is **strict writes in degraded mode**!

---

## ‚úÖ My Strong Recommendation: Option A

**Why Option A (Strict) is Best**:

1. **Industry Standard**: Matches hardware RAID 3 and Linux MD default
2. **Data Safety**: Can't create partial/corrupted files
3. **Simple**: No complex rollback logic needed
4. **Predictable**: Clear error when backend unavailable
5. **Self-Healing Works**: For reads, which is the common case
6. **Production Safe**: Conservative approach

**Trade-off**:
- Writes fail when backend unavailable
- **But this is expected RAID behavior!**
- User should fix the backend, not work around it

---

## üé≠ Real-World Scenario

**User has 3 backends: local, S3, Dropbox**

**Scenario 1: S3 is down**

**With Strict (Option A)**:
```bash
$ rclone copy /data/newfile.txt level3:
ERROR: Failed to upload newfile.txt: odd backend unavailable

# User fixes S3, retries
$ rclone copy /data/newfile.txt level3:
SUCCESS: All 3 particles uploaded
```

**With Degraded Writes (Your proposal)**:
```bash
$ rclone copy /data/newfile.txt level3:
SUCCESS: Uploaded to 2 of 3 backends (odd missing)

# File exists but is immediately degraded
$ rclone ls level3:
INFO: Reconstructing newfile.txt from even+parity (degraded)

# 100 files later...
# All 100 files need reconstruction on every read!
```

**Which is better?**
- Strict: User knows there's a problem, fixes it
- Degraded: Silent degradation, performance impact

---

## üéØ Final Recommendation

**Go with Option A (Strict)**:

‚úÖ **Reads**: Best effort (2 of 3) - already implemented  
‚úÖ **Deletes**: Best effort (idempotent) - already implemented  
‚ùå **Writes/Moves**: Atomic (all 3 required) - to be enforced  

**This matches hardware RAID 3 behavior and is production-safe!**

---

## üìù If You Still Prefer Partial Success...

I can implement it, but I'd recommend:

1. **Make it configurable** (Option B, not Option C)
2. **Default to strict** (safe for most users)
3. **Document trade-offs clearly**
4. **Add warnings** when operating in degraded write mode

**Example**:
```
$ rclone copy file.txt level3: --level3-write-policy degraded
WARNING: Operating in degraded write mode - new files will require reconstruction
INFO: Uploaded to 2 of 3 backends (odd unavailable)
INFO: Queued odd particle for self-healing
```

---

**What's your preference?** ü§î

**Option A**: Strict (like hardware RAID 3) - simple, safe, recommended  
**Option B**: Configurable (default strict, optional degraded) - flexible  
**Option C**: Always allow degraded writes - highest availability, most complex  

I personally vote for **Option A** because it's the safest and matches industry standards, but I'm happy to implement whichever you choose!

